input {
  # Consume job.upserted events from Kafka
  kafka {
    bootstrap_servers => "kafka:29092"
    topics => ["job.upserted"]
    group_id => "logstash-jobs-consumer"
    codec => "json"
    consumer_threads => 1
    decorate_events => "basic"
    auto_offset_reset => "earliest"
  }
}

input {
  # Consume job.deleted events from Kafka  
  kafka {
    bootstrap_servers => "kafka:29092"
    topics => ["job.deleted"]
    group_id => "logstash-jobs-consumer"
    codec => "json"
    consumer_threads => 1
    decorate_events => "basic"
    auto_offset_reset => "earliest"
    tags => ["delete"]
  }
}

filter {
  # Parse the JSON message if needed
  if [message] {
    json {
      source => "message"
      target => "job_data"
    }
  }
  
  # Add a timestamp if not present
  if ![timestamp] {
    mutate {
      add_field => { "indexed_at" => "%{@timestamp}" }
    }
  }
  
  # Remove Kafka metadata fields we don't need in ES
  mutate {
    remove_field => ["@version", "message"]
  }
}

output {
  # Handle delete events
  if "delete" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "jobs-logstash"
      action => "delete"
      document_id => "%{[id]}"
    }
  } else {
    # Handle upsert events
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "jobs-logstash"
      action => "index"
      document_id => "%{[id]}"
    }
  }
  
  # Debug output to stdout
  stdout {
    codec => rubydebug
  }
}
